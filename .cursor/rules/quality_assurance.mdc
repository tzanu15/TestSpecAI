---
description: Quality Assurance and Testing Guidelines for TestSpecAI
globs: **/*
alwaysApply: true
---

# Quality Assurance and Testing Guidelines

## **Testing Strategy Overview**

### **✅ Testing Pyramid**
```
    /\
   /  \     E2E Tests (5%)
  /____\    
 /      \   Integration Tests (15%)
/________\  
/          \ Unit Tests (80%)
/____________\
```

### **✅ Test Coverage Requirements**
- **Backend**: Minimum 80% code coverage
- **Frontend**: Minimum 70% code coverage
- **Critical paths**: 100% coverage (AI services, document processing)
- **API endpoints**: 100% coverage

## **Backend Testing (CRITICAL RULES)**

### **✅ Test Structure**
```
backend/tests/
├── __init__.py
├── conftest.py              # Pytest configuration and fixtures
├── test_models/             # Model tests
│   ├── test_requirement.py
│   ├── test_test_spec.py
│   ├── test_parameter.py
│   └── test_command.py
├── test_crud/               # CRUD operation tests
│   ├── test_requirement_crud.py
│   ├── test_test_spec_crud.py
│   └── test_parameter_crud.py
├── test_api/                # API endpoint tests
│   ├── test_requirements_api.py
│   ├── test_test_specs_api.py
│   └── test_parameters_api.py
├── test_services/           # Service layer tests
│   ├── test_nlp_service.py
│   ├── test_llm_service.py
│   └── test_document_service.py
├── test_utils/              # Utility function tests
└── fixtures/                # Test data fixtures
    ├── sample_requirements.docx
    ├── sample_requirements.pdf
    └── test_data.json
```

### **✅ Pytest Configuration**
```python
# backend/tests/conftest.py
import pytest
import asyncio
from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession
from sqlalchemy.orm import sessionmaker
from httpx import AsyncClient
from app.main import app
from app.database import get_db
from app.models.base import Base

# Test database URL
TEST_DATABASE_URL = "sqlite+aiosqlite:///./test.db"

# Create test engine
test_engine = create_async_engine(TEST_DATABASE_URL, echo=False)
TestSessionLocal = sessionmaker(test_engine, class_=AsyncSession, expire_on_commit=False)

@pytest.fixture(scope="session")
def event_loop():
    """Create an instance of the default event loop for the test session."""
    loop = asyncio.get_event_loop_policy().new_event_loop()
    yield loop
    loop.close()

@pytest.fixture(scope="function")
async def db_session():
    """Create a fresh database session for each test."""
    # Create tables
    async with test_engine.begin() as conn:
        await conn.run_sync(Base.metadata.create_all)
    
    # Create session
    async with TestSessionLocal() as session:
        yield session
    
    # Drop tables
    async with test_engine.begin() as conn:
        await conn.run_sync(Base.metadata.drop_all)

@pytest.fixture(scope="function")
async def client(db_session):
    """Create test client with database session override."""
    def override_get_db():
        yield db_session
    
    app.dependency_overrides[get_db] = override_get_db
    
    async with AsyncClient(app=app, base_url="http://test") as ac:
        yield ac
    
    app.dependency_overrides.clear()

@pytest.fixture
def sample_requirement_data():
    """Sample requirement data for testing."""
    return {
        "title": "Test Requirement",
        "description": "This is a test requirement for unit testing",
        "category_id": "test-category-id",
        "source": "manual",
        "metadata": {"test": True}
    }

@pytest.fixture
def sample_test_spec_data():
    """Sample test specification data for testing."""
    return {
        "name": "Test Specification",
        "description": "This is a test specification for unit testing",
        "requirement_ids": ["test-req-1", "test-req-2"],
        "precondition": "System is initialized",
        "test_steps": [
            {
                "action": {"command_id": "test-cmd-1", "populated_parameters": {}},
                "expected_result": {"command_id": "test-cmd-2", "populated_parameters": {}},
                "description": "Test step description",
                "sequence_number": 1
            }
        ],
        "postcondition": "Test completed successfully",
        "functional_area": "UDS"
    }
```

### **✅ Model Tests**
```python
# backend/tests/test_models/test_requirement.py
import pytest
from sqlalchemy.ext.asyncio import AsyncSession
from app.models.requirement import Requirement
from app.models.category import RequirementCategory

@pytest.mark.asyncio
async def test_requirement_creation(db_session: AsyncSession):
    """Test requirement model creation"""
    category = RequirementCategory(
        name="Test Category",
        description="Test category description",
        created_by="test-user"
    )
    db_session.add(category)
    await db_session.commit()
    
    requirement = Requirement(
        title="Test Requirement",
        description="Test requirement description",
        category_id=category.id,
        created_by="test-user"
    )
    
    db_session.add(requirement)
    await db_session.commit()
    await db_session.refresh(requirement)
    
    assert requirement.id is not None
    assert requirement.title == "Test Requirement"
    assert requirement.category_id == category.id
    assert requirement.is_active is True

@pytest.mark.asyncio
async def test_requirement_relationships(db_session: AsyncSession):
    """Test requirement model relationships"""
    category = RequirementCategory(
        name="Test Category",
        description="Test category description",
        created_by="test-user"
    )
    db_session.add(category)
    await db_session.commit()
    
    requirement = Requirement(
        title="Test Requirement",
        description="Test requirement description",
        category_id=category.id,
        created_by="test-user"
    )
    
    db_session.add(requirement)
    await db_session.commit()
    await db_session.refresh(requirement)
    
    # Test relationship
    assert requirement.category.name == "Test Category"
```

### **✅ API Tests**
```python
# backend/tests/test_api/test_requirements_api.py
import pytest
from httpx import AsyncClient
from app.models.requirement import Requirement
from app.models.category import RequirementCategory

@pytest.mark.asyncio
async def test_create_requirement(client: AsyncClient, db_session):
    """Test requirement creation via API"""
    # Create category first
    category = RequirementCategory(
        name="Test Category",
        description="Test category description",
        created_by="test-user"
    )
    db_session.add(category)
    await db_session.commit()
    
    # Create requirement
    response = await client.post(
        "/api/v1/requirements/",
        json={
            "title": "Test Requirement",
            "description": "Test requirement description",
            "category_id": str(category.id),
            "source": "manual"
        }
    )
    
    assert response.status_code == 200
    data = response.json()
    assert data["title"] == "Test Requirement"
    assert data["category_id"] == str(category.id)

@pytest.mark.asyncio
async def test_get_requirements(client: AsyncClient, db_session):
    """Test getting requirements via API"""
    # Create test data
    category = RequirementCategory(
        name="Test Category",
        description="Test category description",
        created_by="test-user"
    )
    db_session.add(category)
    await db_session.commit()
    
    requirement = Requirement(
        title="Test Requirement",
        description="Test requirement description",
        category_id=category.id,
        created_by="test-user"
    )
    db_session.add(requirement)
    await db_session.commit()
    
    # Get requirements
    response = await client.get("/api/v1/requirements/")
    
    assert response.status_code == 200
    data = response.json()
    assert len(data) == 1
    assert data[0]["title"] == "Test Requirement"

@pytest.mark.asyncio
async def test_update_requirement(client: AsyncClient, db_session):
    """Test requirement update via API"""
    # Create test data
    category = RequirementCategory(
        name="Test Category",
        description="Test category description",
        created_by="test-user"
    )
    db_session.add(category)
    await db_session.commit()
    
    requirement = Requirement(
        title="Test Requirement",
        description="Test requirement description",
        category_id=category.id,
        created_by="test-user"
    )
    db_session.add(requirement)
    await db_session.commit()
    
    # Update requirement
    response = await client.put(
        f"/api/v1/requirements/{requirement.id}",
        json={
            "title": "Updated Requirement",
            "description": "Updated description"
        }
    )
    
    assert response.status_code == 200
    data = response.json()
    assert data["title"] == "Updated Requirement"

@pytest.mark.asyncio
async def test_delete_requirement(client: AsyncClient, db_session):
    """Test requirement deletion via API"""
    # Create test data
    category = RequirementCategory(
        name="Test Category",
        description="Test category description",
        created_by="test-user"
    )
    db_session.add(category)
    await db_session.commit()
    
    requirement = Requirement(
        title="Test Requirement",
        description="Test requirement description",
        category_id=category.id,
        created_by="test-user"
    )
    db_session.add(requirement)
    await db_session.commit()
    
    # Delete requirement
    response = await client.delete(f"/api/v1/requirements/{requirement.id}")
    
    assert response.status_code == 200
    assert response.json()["message"] == "Requirement deleted successfully"

@pytest.mark.asyncio
async def test_requirement_not_found(client: AsyncClient):
    """Test getting non-existent requirement"""
    response = await client.get("/api/v1/requirements/non-existent-id")
    
    assert response.status_code == 404
    assert "not found" in response.json()["detail"].lower()
```

### **✅ Service Tests**
```python
# backend/tests/test_services/test_nlp_service.py
import pytest
import numpy as np
from app.services.nlp_service import nlp_service

@pytest.mark.asyncio
async def test_embedding_generation():
    """Test embedding generation"""
    texts = ["This is a test requirement", "Another test requirement"]
    embeddings = await nlp_service.generate_embeddings(texts)
    
    assert embeddings.shape == (2, 384)  # 384 dimensions for all-MiniLM-L6-v2
    assert not np.isnan(embeddings).any()
    assert not np.isinf(embeddings).any()

@pytest.mark.asyncio
async def test_similarity_calculation():
    """Test similarity calculation"""
    text1 = "The system shall provide diagnostic capabilities"
    text2 = "The system must offer diagnostic functionality"
    
    embeddings = await nlp_service.generate_embeddings([text1, text2])
    similarity = nlp_service._calculate_similarity(embeddings[0], embeddings[1])
    
    assert 0 <= similarity <= 1
    assert similarity > 0.5  # Should be similar

@pytest.mark.asyncio
async def test_confidence_calculation():
    """Test confidence level calculation"""
    assert nlp_service._calculate_confidence(0.95) == "high"
    assert nlp_service._calculate_confidence(0.8) == "medium"
    assert nlp_service._calculate_confidence(0.6) == "low"
```

## **Frontend Testing (CRITICAL RULES)**

### **✅ Test Structure**
```
frontend/src/
├── __tests__/               # Test files
│   ├── components/          # Component tests
│   │   ├── RequirementsList.test.tsx
│   │   ├── TestSpecEditor.test.tsx
│   │   └── ParameterManager.test.tsx
│   ├── hooks/               # Hook tests
│   │   ├── useRequirements.test.ts
│   │   └── useForm.test.ts
│   ├── services/            # Service tests
│   │   └── api.test.ts
│   ├── stores/              # Store tests
│   │   └── requirementsStore.test.ts
│   └── utils/               # Utility tests
│       └── helpers.test.ts
├── test-utils/              # Test utilities
│   ├── render.tsx           # Custom render function
│   ├── mocks/               # Mock implementations
│   └── fixtures/            # Test data
└── setupTests.ts            # Test setup
```

### **✅ Test Setup**
```typescript
// frontend/src/setupTests.ts
import '@testing-library/jest-dom'
import { server } from './test-utils/mocks/server'

// Establish API mocking before all tests
beforeAll(() => server.listen())

// Reset any request handlers that we may add during the tests
afterEach(() => server.resetHandlers())

// Clean up after the tests are finished
afterAll(() => server.close())
```

### **✅ Custom Render Function**
```typescript
// frontend/src/test-utils/render.tsx
import React, { ReactElement } from 'react'
import { render, RenderOptions } from '@testing-library/react'
import { BrowserRouter } from 'react-router-dom'
import { ConfigProvider } from 'antd'
import { QueryClient, QueryClientProvider } from '@tanstack/react-query'

const AllTheProviders = ({ children }: { children: React.ReactNode }) => {
  const queryClient = new QueryClient({
    defaultOptions: {
      queries: {
        retry: false,
      },
    },
  })

  return (
    <QueryClientProvider client={queryClient}>
      <ConfigProvider>
        <BrowserRouter>
          {children}
        </BrowserRouter>
      </ConfigProvider>
    </QueryClientProvider>
  )
}

const customRender = (
  ui: ReactElement,
  options?: Omit<RenderOptions, 'wrapper'>,
) => render(ui, { wrapper: AllTheProviders, ...options })

export * from '@testing-library/react'
export { customRender as render }
```

### **✅ Component Tests**
```typescript
// frontend/src/__tests__/components/RequirementsList.test.tsx
import React from 'react'
import { screen, fireEvent, waitFor } from '@testing-library/react'
import { render } from '../../test-utils/render'
import { RequirementsList } from '../../components/requirements/RequirementsList'
import { Requirement } from '../../types/requirements'

const mockRequirements: Requirement[] = [
  {
    id: '1',
    title: 'Test Requirement 1',
    description: 'Test description 1',
    category_id: 'test-category-1',
    source: 'manual',
    metadata: {},
    created_at: '2023-01-01T00:00:00Z',
    updated_at: '2023-01-01T00:00:00Z',
    created_by: 'test-user',
    is_active: true,
  },
  {
    id: '2',
    title: 'Test Requirement 2',
    description: 'Test description 2',
    category_id: 'test-category-2',
    source: 'document',
    metadata: {},
    created_at: '2023-01-01T00:00:00Z',
    updated_at: '2023-01-01T00:00:00Z',
    created_by: 'test-user',
    is_active: true,
  },
]

const mockProps = {
  requirements: mockRequirements,
  loading: false,
  onEdit: jest.fn(),
  onDelete: jest.fn(),
}

describe('RequirementsList', () => {
  beforeEach(() => {
    jest.clearAllMocks()
  })

  it('renders requirements list', () => {
    render(<RequirementsList {...mockProps} />)
    
    expect(screen.getByText('Test Requirement 1')).toBeInTheDocument()
    expect(screen.getByText('Test Requirement 2')).toBeInTheDocument()
  })

  it('displays loading state', () => {
    render(<RequirementsList {...mockProps} loading={true} />)
    
    expect(screen.getByRole('table')).toBeInTheDocument()
    // Ant Design Table shows loading spinner
  })

  it('calls onEdit when edit button is clicked', async () => {
    render(<RequirementsList {...mockProps} />)
    
    const editButtons = screen.getAllByText('Edit')
    fireEvent.click(editButtons[0])
    
    expect(mockProps.onEdit).toHaveBeenCalledWith('1', mockRequirements[0])
  })

  it('calls onDelete when delete button is clicked', async () => {
    render(<RequirementsList {...mockProps} />)
    
    const deleteButtons = screen.getAllByText('Delete')
    fireEvent.click(deleteButtons[0])
    
    // Confirm deletion in popconfirm
    const confirmButton = screen.getByText('Yes')
    fireEvent.click(confirmButton)
    
    expect(mockProps.onDelete).toHaveBeenCalledWith('1')
  })

  it('displays source tags correctly', () => {
    render(<RequirementsList {...mockProps} />)
    
    expect(screen.getByText('manual')).toBeInTheDocument()
    expect(screen.getByText('document')).toBeInTheDocument()
  })

  it('handles empty requirements list', () => {
    render(<RequirementsList {...mockProps} requirements={[]} />)
    
    expect(screen.getByRole('table')).toBeInTheDocument()
    // Table should be empty but still render
  })
})
```

### **✅ Hook Tests**
```typescript
// frontend/src/__tests__/hooks/useRequirements.test.ts
import { renderHook, waitFor } from '@testing-library/react'
import { QueryClient, QueryClientProvider } from '@tanstack/react-query'
import { useRequirements } from '../../hooks/useRequirements'
import { requirementsService } from '../../services/requirements'

// Mock the service
jest.mock('../../services/requirements')
const mockRequirementsService = requirementsService as jest.Mocked<typeof requirementsService>

const createWrapper = () => {
  const queryClient = new QueryClient({
    defaultOptions: {
      queries: {
        retry: false,
      },
    },
  })
  
  return ({ children }: { children: React.ReactNode }) => (
    <QueryClientProvider client={queryClient}>
      {children}
    </QueryClientProvider>
  )
}

describe('useRequirements', () => {
  beforeEach(() => {
    jest.clearAllMocks()
  })

  it('loads requirements on mount', async () => {
    const mockRequirements = [
      {
        id: '1',
        title: 'Test Requirement',
        description: 'Test description',
        category_id: 'test-category',
        source: 'manual',
        metadata: {},
        created_at: '2023-01-01T00:00:00Z',
        updated_at: '2023-01-01T00:00:00Z',
        created_by: 'test-user',
        is_active: true,
      },
    ]

    mockRequirementsService.getRequirements.mockResolvedValue(mockRequirements)

    const { result } = renderHook(() => useRequirements(), {
      wrapper: createWrapper(),
    })

    await waitFor(() => {
      expect(result.current.requirements).toEqual(mockRequirements)
    })

    expect(mockRequirementsService.getRequirements).toHaveBeenCalled()
  })

  it('handles loading state', async () => {
    mockRequirementsService.getRequirements.mockImplementation(
      () => new Promise(resolve => setTimeout(resolve, 100))
    )

    const { result } = renderHook(() => useRequirements(), {
      wrapper: createWrapper(),
    })

    expect(result.current.loading).toBe(true)

    await waitFor(() => {
      expect(result.current.loading).toBe(false)
    })
  })

  it('handles error state', async () => {
    const errorMessage = 'Failed to load requirements'
    mockRequirementsService.getRequirements.mockRejectedValue(new Error(errorMessage))

    const { result } = renderHook(() => useRequirements(), {
      wrapper: createWrapper(),
    })

    await waitFor(() => {
      expect(result.current.error).toBe(errorMessage)
    })
  })

  it('creates requirement successfully', async () => {
    const newRequirement = {
      title: 'New Requirement',
      description: 'New description',
      category_id: 'test-category',
    }

    const createdRequirement = {
      id: '2',
      ...newRequirement,
      source: 'manual',
      metadata: {},
      created_at: '2023-01-01T00:00:00Z',
      updated_at: '2023-01-01T00:00:00Z',
      created_by: 'test-user',
      is_active: true,
    }

    mockRequirementsService.createRequirement.mockResolvedValue(createdRequirement)

    const { result } = renderHook(() => useRequirements(), {
      wrapper: createWrapper(),
    })

    await waitFor(() => {
      expect(result.current.createRequirement).toBeDefined()
    })

    const created = await result.current.createRequirement(newRequirement)

    expect(created).toEqual(createdRequirement)
    expect(mockRequirementsService.createRequirement).toHaveBeenCalledWith(newRequirement)
  })
})
```

## **Integration Testing**

### **✅ API Integration Tests**
```python
# backend/tests/test_integration/test_api_integration.py
import pytest
from httpx import AsyncClient
from app.main import app

@pytest.mark.asyncio
async def test_full_requirement_workflow():
    """Test complete requirement workflow"""
    async with AsyncClient(app=app, base_url="http://test") as client:
        # 1. Create category
        category_response = await client.post(
            "/api/v1/requirements/categories",
            json={
                "name": "Test Category",
                "description": "Test category description"
            }
        )
        assert category_response.status_code == 200
        category_id = category_response.json()["id"]
        
        # 2. Create requirement
        requirement_response = await client.post(
            "/api/v1/requirements/",
            json={
                "title": "Test Requirement",
                "description": "Test requirement description",
                "category_id": category_id,
                "source": "manual"
            }
        )
        assert requirement_response.status_code == 200
        requirement_id = requirement_response.json()["id"]
        
        # 3. Get requirement
        get_response = await client.get(f"/api/v1/requirements/{requirement_id}")
        assert get_response.status_code == 200
        assert get_response.json()["title"] == "Test Requirement"
        
        # 4. Update requirement
        update_response = await client.put(
            f"/api/v1/requirements/{requirement_id}",
            json={
                "title": "Updated Requirement",
                "description": "Updated description"
            }
        )
        assert update_response.status_code == 200
        assert update_response.json()["title"] == "Updated Requirement"
        
        # 5. Delete requirement
        delete_response = await client.delete(f"/api/v1/requirements/{requirement_id}")
        assert delete_response.status_code == 200
        
        # 6. Verify deletion
        get_deleted_response = await client.get(f"/api/v1/requirements/{requirement_id}")
        assert get_deleted_response.status_code == 404
```

## **Performance Testing**

### **✅ Load Testing**
```python
# backend/tests/test_performance/test_load.py
import pytest
import asyncio
import time
from httpx import AsyncClient
from app.main import app

@pytest.mark.asyncio
async def test_concurrent_requirement_creation():
    """Test concurrent requirement creation"""
    async with AsyncClient(app=app, base_url="http://test") as client:
        # Create category first
        category_response = await client.post(
            "/api/v1/requirements/categories",
            json={
                "name": "Load Test Category",
                "description": "Category for load testing"
            }
        )
        category_id = category_response.json()["id"]
        
        # Create multiple requirements concurrently
        async def create_requirement(index):
            response = await client.post(
                "/api/v1/requirements/",
                json={
                    "title": f"Load Test Requirement {index}",
                    "description": f"Load test requirement {index}",
                    "category_id": category_id,
                    "source": "load_test"
                }
            )
            return response.status_code == 200
        
        start_time = time.time()
        tasks = [create_requirement(i) for i in range(100)]
        results = await asyncio.gather(*tasks)
        end_time = time.time()
        
        # Verify all requests succeeded
        assert all(results)
        
        # Verify performance (should complete within 10 seconds)
        assert end_time - start_time < 10.0
```

## **Quality Assurance Checklist**

### **Before Marking Any Task Complete:**
- [ ] **All unit tests pass** (backend and frontend)
- [ ] **Integration tests pass**
- [ ] **Code coverage meets requirements** (80% backend, 70% frontend)
- [ ] **No linting errors** (ESLint, Pylint, Black)
- [ ] **No security vulnerabilities** (bandit, npm audit)
- [ ] **Performance tests pass**
- [ ] **Documentation is updated**
- [ ] **Error handling is comprehensive**
- [ ] **Logging is properly implemented**
- [ ] **Configuration is environment-specific**

### **Code Quality Checks:**
- [ ] **Type hints** are used everywhere (Python/TypeScript)
- [ ] **Error handling** covers all edge cases
- [ ] **Input validation** is implemented
- [ ] **SQL injection** prevention is in place
- [ ] **XSS prevention** is implemented
- [ ] **CSRF protection** is in place
- [ ] **Rate limiting** is implemented
- [ ] **Proper logging** without sensitive data
- [ ] **Resource cleanup** is handled
- [ ] **Memory leaks** are prevented

### **Security Checklist:**
- [ ] **Authentication** is properly implemented (if applicable)
- [ ] **Authorization** checks are in place
- [ ] **Input sanitization** prevents injection attacks
- [ ] **File upload** validation is comprehensive
- [ ] **API endpoints** are properly secured
- [ ] **Sensitive data** is not logged
- [ ] **Environment variables** are used for secrets
- [ ] **HTTPS** is enforced in production
- [ ] **CORS** is properly configured
- [ ] **Security headers** are set

---

**Remember**: Quality assurance is not optional. Every piece of code must be thoroughly tested and validated before being marked complete. This ensures the reliability and security of the TestSpecAI application.